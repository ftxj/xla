[FTXJ LOG] InitXlaModuleBindings
[FTXJ LOG] _xla_set_default_device
[FTXJ LOG] _xla_set_default_device call SetCurrentThreadDevice
[FTXJ LOG] SetCurrentThreadDevice xla:0
[FTXJ LOG] SetCurrentThreadDevice call bridge::SetCurrentDevice. do data format transfer (string -> c10::Device)
[FTXJ LOG] bridge::SetCurrentDevice. input=c10:Device
[FTXJ LOG] AtenDeviceToXlaDevice. input = c10::Device, output = lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice call AtenXlaDeviceMapper::Get()->GetDeviceFromOrdinal
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] AtenXlaDeviceMapper init. need call xla::ComputationClient Get()
[FTXH LOG] ComputationClient::Get. singleton
[CreateClient] computation_client.cpp file
[ComputationClient::Create] only can create Xrt Computation Client
[ParseEnvBasedTpuClusterConfig] this function need kEnvTpuConfig
[XrtComputationClient] Init....
[FTXJ LOG] xla_client::ShouldStartLocalService
[FTXJ LOG] xla_client::ShouldStartLocalService End true
[FTXJ LOG] DeviceType default construction CPU
[GetDeviceFromOrdinal] from device index to lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] DeviceType default construction CPU
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[FTXJ LOG] bridge::SetCurrentDevice End
[FTXJ LOG] SetCurrentThreadDevice call end.
[FTXJ LOG] SetCurrentThreadDevice End.
[FTXJ LOG] _xla_set_default_device End


[FTXJ LOG autogen] THPVariable_randn
[FTXJ LOG] THPVariable_randn call dispatch
[FTXJ LOG] dispatch overload index = 2
[FTXJ LOG] autogen code call_dispatch
[FTXJ LOG] call wrap(dispatch_randn)
[FTXJ LOG] lambda dispatch_randn
[FTXJ LOG] lambda dispatch_randn call torch::randn with (size, options)
[FTXJ LOG] variable_factor randn call autograd::make_variable(at::randn)
[FTXJ LOG] aten::randn
[FTXJ LOG] aten::randn call at::_ops::randn::call()
[FTXJ LOG] create_randn_typed_handle
[FTXJ LOG] create_randn_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_randn_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(BackendSelect)
[FTXJ LOG] create_randn_typed_handle
[FTXJ LOG] create_randn_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_randn_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] aten::empty
[FTXJ LOG] aten::empty call at::_ops::empty_memory_format::call()
[FTXJ LOG] create_empty_memory_format_typed_handle
[FTXJ LOG] create_empty_memory_format_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_empty_memory_format_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(BackendSelect)
[FTXJ LOG] create_empty_memory_format_typed_handle
[FTXJ LOG] create_empty_memory_format_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_empty_memory_format_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] XLANativeFunctions::empty
[FTXJ LOG] XLANativeFunctions::empty call     AtenFromXlaTensor->XLATensor::full->GetXlaDeviceOrCurrent
[FTXJ LOG] GetXlaDeviceOrCurrent
[FTXJ LOG] GetXlaDeviceOrCurrent call GetXlaDevice
[FTXJ LOG] GetXlaDevice.device.opt
[FTXJ LOG] GetXlaDevice.device
[FTXJ LOG] AtenDeviceToXlaDevice. input = c10::Device, output = lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice call AtenXlaDeviceMapper::Get()->GetDeviceFromOrdinal
[FTXJ LOG] AtenXlaDeviceMapper::Get
[GetDeviceFromOrdinal] from device index to lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice End
[FTXJ LOG] GetXlaDevice.device End
[FTXJ LOG] GetXlaDevice.device.opt End
[FTXJ LOG] GetXlaDeviceOrCurrent may call GetCurrentDevice
[FTXJ LOG] GetXlaDeviceOrCurrent End
[FTXJ LOG] XLATensor::full
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] InferOutputShape
[FTXJ LOG] XlaBuilder InferOutputShape
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] [XLATensor::Create Begin] from ir::XlaValue
[FTXJ LOG] [XLA Tensor] Constructor from ir::XlaValue
[FTXJ LOG] Register Tensor to Device. push data into device vector
[FTXJ LOG] [XLATensor::Create End]
[FTXJ LOG] XLATensor::full End
[AtenFromXlaTensor] XLATensor -> Tensor
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[XLATensorImpl] constructor
[FTXJ LOG] XLANativeFunctions::empty End
[FTXJ LOG] create_normal__typed_handle
[FTXJ LOG] create_normal__typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_normal__typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA)
[GetXlaTensor] convert from at::Tensor -> XLATensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] InferOutputShape
[FTXJ LOG] XlaBuilder InferOutputShape
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] DeviceContextArena::GetRngSeed
[FTXJ LOG] aten::scalar_tensor
[FTXJ LOG] aten::scalar_tensor call at::_ops::scalar_tensor::call()
[FTXJ LOG] create_scalar_tensor_typed_handle
[FTXJ LOG] create_scalar_tensor_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_scalar_tensor_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(BackendSelect)
[FTXJ LOG] create_scalar_tensor_typed_handle
[FTXJ LOG] create_scalar_tensor_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_scalar_tensor_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(CPU)
[FTXH LOG] ComputationClient::Get. singleton
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] NodePtr::Add Constructor
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] DeviceContextArena::GetRngSeed End
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::SetInPlaceIrValue
[FTXJ LOG] XLATensor::SetIrValue
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetIrValue End
[FTXJ LOG] XLATensor::SetInPlaceIrValue End
[FTXJ LOG] variable_factor randn call end
[FTXJ LOG] lambda dispatch_randn End
[FTXJ LOG] wrap Tensor->PyObject*
[FTXJ LOG] call wrap(dispatch_randn) End
[FTXJ LOG autogen] THPVariable_randn
[FTXJ LOG] THPVariable_randn call dispatch
[FTXJ LOG] dispatch overload index = 2
[FTXJ LOG] autogen code call_dispatch
[FTXJ LOG] call wrap(dispatch_randn)
[FTXJ LOG] lambda dispatch_randn
[FTXJ LOG] lambda dispatch_randn call torch::randn with (size, options)
[FTXJ LOG] variable_factor randn call autograd::make_variable(at::randn)
[FTXJ LOG] aten::randn
[FTXJ LOG] aten::randn call at::_ops::randn::call()
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(BackendSelect)
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] aten::empty
[FTXJ LOG] aten::empty call at::_ops::empty_memory_format::call()
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(BackendSelect)
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] XLANativeFunctions::empty
[FTXJ LOG] XLANativeFunctions::empty call     AtenFromXlaTensor->XLATensor::full->GetXlaDeviceOrCurrent
[FTXJ LOG] GetXlaDeviceOrCurrent
[FTXJ LOG] GetXlaDeviceOrCurrent call GetXlaDevice
[FTXJ LOG] GetXlaDevice.device.opt
[FTXJ LOG] GetXlaDevice.device
[FTXJ LOG] AtenDeviceToXlaDevice. input = c10::Device, output = lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice call AtenXlaDeviceMapper::Get()->GetDeviceFromOrdinal
[FTXJ LOG] AtenXlaDeviceMapper::Get
[GetDeviceFromOrdinal] from device index to lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice End
[FTXJ LOG] GetXlaDevice.device End
[FTXJ LOG] GetXlaDevice.device.opt End
[FTXJ LOG] GetXlaDeviceOrCurrent may call GetCurrentDevice
[FTXJ LOG] GetXlaDeviceOrCurrent End
[FTXJ LOG] XLATensor::full
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] [XLATensor::Create Begin] from ir::XlaValue
[FTXJ LOG] [XLA Tensor] Constructor from ir::XlaValue
[FTXJ LOG] Register Tensor to Device. push data into device vector
[FTXJ LOG] [XLATensor::Create End]
[FTXJ LOG] XLATensor::full End
[AtenFromXlaTensor] XLATensor -> Tensor
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[XLATensorImpl] constructor
[FTXJ LOG] XLANativeFunctions::empty End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA)
[GetXlaTensor] convert from at::Tensor -> XLATensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] DeviceContextArena::GetRngSeed
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] NodePtr::Add Constructor
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] DeviceContextArena::GetRngSeed End
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::SetInPlaceIrValue
[FTXJ LOG] XLATensor::SetIrValue
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetIrValue End
[FTXJ LOG] XLATensor::SetInPlaceIrValue End
[FTXJ LOG] variable_factor randn call end
[FTXJ LOG] lambda dispatch_randn End
[FTXJ LOG] wrap Tensor->PyObject*
[FTXJ LOG] call wrap(dispatch_randn) End
[FTXJ LOG autogen] THPVariable_relu
[FTXJ LOG] THPVariable_relu
[FTXJ LOG] THPVariable_relu call dispatch
[FTXJ LOG] call wrap(dispatch_relu)
[FTXJ LOG] lambda dispatch_relu
[FTXJ LOG] lambda dispatch_relu call self.relu with ()
[FTXJ LOG] create_relu_typed_handle
[FTXJ LOG] create_relu_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_relu_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA, AutogradXLA)
[FTXJ LOG] create_relu_typed_handle
[FTXJ LOG] create_relu_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_relu_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[GetXlaTensor] convert from at::Tensor -> XLATensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] InferOutputShape
[FTXJ LOG] XlaBuilder InferOutputShape
[FTXJ LOG] BuildRelu
[FTXJ LOG] BuildRelu call ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] BuildRelu call ShapeOfXlaOp result is
element_type: F32
dimensions: 2
dimensions: 2
layout {
  minor_to_major: 1
  minor_to_major: 0
  format: DENSE
}
is_dynamic_dimension: false
is_dynamic_dimension: false

[FTXJ LOG] BuildRelu call xla::Max, add inst to Builder
[FTXJ LOG] End BuildRelu
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaValue construct
[XLATensor::CreateFrom] ir::XlaValue
[FTXJ LOG] XLATensor::MaybeCastIrValue
[FTXJ LOG] XLATensor::MaybeCastIrValue End
[FTXJ LOG] [XLATensor::Create Begin] from ir::XlaValue
[FTXJ LOG] [XLA Tensor] Constructor from ir::XlaValue
[FTXJ LOG] Register Tensor to Device. push data into device vector
[FTXJ LOG] [XLATensor::Create End]
[AtenFromXlaTensor] XLATensor -> Tensor
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[XLATensorImpl] constructor
[FTXJ LOG] lambda dispatch_relu End
[FTXJ LOG] wrap Tensor->PyObject*
[FTXJ LOG] call wrap(dispatch_relu) End
[FTXJ LOG autogen] THPVariable_abs
[FTXJ LOG] THPVariable_abs
[FTXJ LOG] THPVariable_abs call dispatch
[FTXJ LOG] autogen code call_dispatch
[FTXJ LOG] call wrap(dispatch_abs)
[FTXJ LOG] lambda dispatch_abs
[FTXJ LOG] lambda dispatch_abs call self.abs with ()
[FTXJ LOG] create_abs_typed_handle
[FTXJ LOG] create_abs_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_abs_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA, AutogradXLA)
[FTXJ LOG] create_abs_typed_handle
[FTXJ LOG] create_abs_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_abs_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[GetXlaTensor] convert from at::Tensor -> XLATensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[XLATensor::CreateFrom] ir::XlaValue
[FTXJ LOG] XLATensor::MaybeCastIrValue
[FTXJ LOG] XLATensor::MaybeCastIrValue End
[FTXJ LOG] [XLATensor::Create Begin] from ir::XlaValue
[FTXJ LOG] [XLA Tensor] Constructor from ir::XlaValue
[FTXJ LOG] Register Tensor to Device. push data into device vector
[FTXJ LOG] [XLATensor::Create End]
[AtenFromXlaTensor] XLATensor -> Tensor
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[XLATensorImpl] constructor
[FTXJ LOG] lambda dispatch_abs End
[FTXJ LOG] wrap Tensor->PyObject*
[FTXJ LOG] call wrap(dispatch_abs) End
[FTXJ LOG autogen] THPVariable_add
[FTXJ LOG] THPVariable_add call dispatch
[FTXJ LOG] dispatch overload index = 1
[FTXJ LOG] call wrap(dispatch_add)
[FTXJ LOG] lambda dispatch_add
[FTXJ LOG] lambda dispatch_add call self.add with (other, alpha)
[FTXJ LOG] create_add_Tensor_typed_handle
[FTXJ LOG] create_add_Tensor_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_add_Tensor_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA, AutogradXLA)
[FTXJ LOG] create_add_Tensor_typed_handle
[FTXJ LOG] create_add_Tensor_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_add_Tensor_typed_handle End
[FTXJ LOG] call op.redispatch
[FTXJ LOG] TypedOperatorHandle::redispatch, DispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] XLANativeFunctions::add
[FTXJ LOG] add accept input from at::Tensor, output to at::Tensor
[FTXJ LOG] aten::result_type
[FTXJ LOG] aten::result_type call at::_ops::result_type_Tensor::call()
[FTXJ LOG] create_result_type_Tensor_typed_handle
[FTXJ LOG] create_result_type_Tensor_typed_handle call Dispatcher::findSchemaOrThrow
[FTXJ LOG] create_result_type_Tensor_typed_handle End
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA)
[DoBinaryOp] from at::Tensor & binary Op
[FTXJ LOG] aten::result_type
[FTXJ LOG] aten::result_type call at::_ops::result_type_Tensor::call()
[FTXJ LOG] call op.call
[FTXJ LOG] TypedOperatorHandle::call
[FTXJ LOG] Dispatcher::call
[FTXJ LOG] Dispatcher::call dispatchKeySet = DispatchKeySet(XLA)
[FTXJ LOG] GetBinaryOperands
[FTXJ LOG] GetBinaryOperands call TryGetXlaTensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[FTXJ LOG] GetBinaryOperands call GetOrCreateXlaTensor
[GetOrCreateXlaTensor] convert at::Tensor & Device -> XLATensor
[TryGetXlaTensor] convert from at::Tensor -> option<XLATensor>
[GetXlaTensorImpl] convert from at::Tensor -> XLATensorImpl
[FTXJ LOG] GetBinaryOperands End
[FTXJ LOG] DoBinaryOp call XLATensor::add
[FTXJ LOG] XLATensor::add
[FTXJ LOG] XLATensor::add call GetIrValueForScalar
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d
[XlaNode] build. Also a Lazy Node.
[XlaNode] build. Also a XlaNode.
[FTXJ LOG] XlaValue construct
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[XlaNode] build. Also a XlaNode. with GetOpShape
[FTXJ LOG] InferOutputShape
[FTXJ LOG] XlaBuilder InferOutputShape
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaValue construct
[FTXJ LOG] XLATensor::GetIrValueForScalar.v.t.d.d End
[FTXJ LOG] XLATensor::add call GetIrValue & CreateFrom
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[FTXJ LOG] NodePtr::Add Constructor
[XlaNode] build. Also a Lazy Node
[XlaNode] build. Also a XlaNode
[FTXJ LOG] XlaValue construct
[XLATensor::CreateFrom] ir::XlaValue & optional<ScalarType>
[FTXJ LOG] XLATensor::MaybeCastIrValue
[FTXJ LOG] XLATensor::MaybeCastIrValue End
[FTXJ LOG] [XLATensor::Create Begin] from ir::XlaValue
[FTXJ LOG] [XLA Tensor] Constructor from ir::XlaValue
[FTXJ LOG] Register Tensor to Device. push data into device vector
[FTXJ LOG] [XLATensor::Create End]
[FTXJ LOG] DoBinaryOp call XLATensor::add End
[AtenFromXlaTensor] XLATensor -> Tensor
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[XLATensorImpl] constructor
[FTXJ LOG] XLANativeFunctions::add End
[FTXJ LOG] lambda dispatch_add End
[FTXJ LOG] wrap Tensor->PyObject*
[FTXJ LOG] call wrap(dispatch_add) End
[GetCurrentAtenDevice]
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[StepMarker] Begin
[FTXJ LOG] AtenDeviceToXlaDevice. input = c10::Device, output = lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice call AtenXlaDeviceMapper::Get()->GetDeviceFromOrdinal
[FTXJ LOG] AtenXlaDeviceMapper::Get
[GetDeviceFromOrdinal] from device index to lazy::BackendDevice
[FTXJ LOG] AtenDeviceToXlaDevice End
[SyncLiveTensorsGraph Begin]
call XLATensor::GetLiveTensors
[FTXJ LOG] DeviceContextArena::GetLiveTensors
[FTXJ LOG] [XLA Tensor] Constructor from Data
[FTXJ LOG] [XLA Tensor] Constructor from Data
[FTXJ LOG] [XLA Tensor] Constructor from Data
[FTXJ LOG] [XLA Tensor] Constructor from Data
[FTXJ LOG] [XLA Tensor] Constructor from Data
[FTXJ LOG] DeviceContextArena::GetLiveTensors, Live Tensor Numbers = 5
call XLATensor::GetLiveTensors End
[SyncTensorsGraph Begin] Tensor size = 5
[FTXJ LOG] SyncTensorsGraphInternal
[FTXJ LOG] SyncTensorsGraphInternal call CollectSyncTensors
[CollectSyncTensors] input tensor size = 5
[CollectSyncTensors] unique_device ele
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] SyncTensorsGraphInternal call RunPostOrder
[FRXJ LOG] XLATensor::RunPostOrder
call ComputePostOrder on node(str=[] aten::normal, op=aten::normal)
[FTXJ LOG] Util::ComputePostOrder
traversal node(str=[] aten::normal, op=aten::normal)
node first traversal
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
first traversal this node, push into queue
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
first traversal this node, push into queue
operands --- node(str=[] aten::add, op=aten::add)
first traversal this node, push into queue
traversal node(str=[] aten::add, op=aten::add)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
first traversal this node, push into queue
operands --- node(str=[] aten::mul, op=aten::mul)
first traversal this node, push into queue
traversal node(str=[] aten::mul, op=aten::mul)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
first traversal this node, push into queue
operands --- node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node second traversal
node into post order
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::mul, op=aten::mul)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
operands --- node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node into post order
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::add, op=aten::add)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
operands --- node(str=[] aten::mul, op=aten::mul)
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node into post order
traversal node(str=[] aten::normal, op=aten::normal)
node second traversal
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
operands --- node(str=[] aten::add, op=aten::add)
node into post order

After ComputePostOrder, return data is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)


after call ComputePostOrder, post order is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
call ComputePostOrder on node(str=[] aten::normal, op=aten::normal)
[FTXJ LOG] Util::ComputePostOrder
traversal node(str=[] aten::normal, op=aten::normal)
node first traversal
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
first traversal this node, push into queue
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
first traversal this node, push into queue
operands --- node(str=[] aten::add, op=aten::add)
first traversal this node, push into queue
traversal node(str=[] aten::add, op=aten::add)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
first traversal this node, push into queue
operands --- node(str=[] aten::mul, op=aten::mul)
first traversal this node, push into queue
traversal node(str=[] aten::mul, op=aten::mul)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
first traversal this node, push into queue
operands --- node(str=[] aten::add, op=aten::add)
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::mul, op=aten::mul)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
operands --- node(str=[] aten::add, op=aten::add)
node into post order
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::add, op=aten::add)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
operands --- node(str=[] aten::mul, op=aten::mul)
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node into post order
traversal node(str=[] aten::normal, op=aten::normal)
node second traversal
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
operands --- node(str=[] aten::add, op=aten::add)
node into post order

After ComputePostOrder, return data is
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)


after call ComputePostOrder, post order is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
call ComputePostOrder on node(str=[] aten::relu, op=aten::relu)
[FTXJ LOG] Util::ComputePostOrder
traversal node(str=[] aten::relu, op=aten::relu)
node first traversal
operands --- node(str=[] aten::normal, op=aten::normal)
traversal node(str=[] aten::relu, op=aten::relu)
node second traversal
operands --- node(str=[] aten::normal, op=aten::normal)
node into post order

After ComputePostOrder, return data is
node(str=[] aten::relu, op=aten::relu)


after call ComputePostOrder, post order is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[] aten::relu, op=aten::relu)
call ComputePostOrder on node(str=[] aten::abs, op=aten::abs)
[FTXJ LOG] Util::ComputePostOrder
traversal node(str=[] aten::abs, op=aten::abs)
node first traversal
operands --- node(str=[] aten::normal, op=aten::normal)
traversal node(str=[] aten::abs, op=aten::abs)
node second traversal
operands --- node(str=[] aten::normal, op=aten::normal)
node into post order

After ComputePostOrder, return data is
node(str=[] aten::abs, op=aten::abs)


after call ComputePostOrder, post order is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[] aten::relu, op=aten::relu)
node(str=[] aten::abs, op=aten::abs)
call ComputePostOrder on node(str=[] aten::add, op=aten::add)
[FTXJ LOG] Util::ComputePostOrder
traversal node(str=[] aten::add, op=aten::add)
node first traversal
operands --- node(str=[] aten::relu, op=aten::relu)
operands --- node(str=[] aten::mul, op=aten::mul)
first traversal this node, push into queue
traversal node(str=[] aten::mul, op=aten::mul)
node first traversal
operands --- node(str=[] aten::abs, op=aten::abs)
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
first traversal this node, push into queue
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node first traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
first traversal this node, push into queue
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node first traversal
traversal node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node second traversal
node into post order
traversal node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node second traversal
operands --- node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node into post order
traversal node(str=[] aten::mul, op=aten::mul)
node second traversal
operands --- node(str=[] aten::abs, op=aten::abs)
operands --- node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node into post order
traversal node(str=[] aten::add, op=aten::add)
node second traversal
operands --- node(str=[] aten::relu, op=aten::relu)
operands --- node(str=[] aten::mul, op=aten::mul)
node into post order

After ComputePostOrder, return data is
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::mul, op=aten::mul)
node(str=[] aten::add, op=aten::add)


after call ComputePostOrder, post order is
node(str=[UNKNOWN_SCALAR[]] xla::device_data, device=CPU:0, op=xla::device_data)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=214013, op=prim::Constant)
node(str=[] aten::mul, op=aten::mul)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=2531011, op=prim::Constant)
node(str=[] aten::add, op=aten::add)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=0, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::normal, op=aten::normal)
node(str=[] aten::relu, op=aten::relu)
node(str=[] aten::abs, op=aten::abs)
node(str=[UNKNOWN_SCALAR[]] prim::Constant, value=1, op=prim::Constant)
node(str=[] aten::expand, size=(2, 2), op=aten::expand)
node(str=[] aten::mul, op=aten::mul)
node(str=[] aten::add, op=aten::add)
[FTXJ LOG] SyncTensorsGraphInternal call SaveTensorsGraphInfo
[FTXJ LOG] SyncTensorsGraphInternal call TryRunCachedSync
[FTXJ LOG] SyncTensorsGraphInternal call Compile
[FTXJ LOG] XLATensor::Compiler.
[FTXJ MSG] using activate tensor and post order to get Complication Result.
[FTXJ LOG] XLATensor::Compiler call build LoweringContext.
[FTXJ LOG] LoweringContext::LoweringContextSyncTensorsGraph
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetParameter.d from ComputationClient::Data
[FTXJ LOG] LoweringContext::GetParameter.d call ComputationClient:GetOpaqueHandle
[FTXJ LOG] LoweringContext::GetParameter.d not find param, build a new parameter
[FTXJ LOG] LoweringContext::GetParameter.d End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] NodePtr::Add Constructor lower_fn
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op0
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op1
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] NodePtr::Add Constructor lower_fn
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op0
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op1
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] BuildRelu
[FTXJ LOG] BuildRelu call ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] BuildRelu call ShapeOfXlaOp result is
element_type: F32
dimensions: 2
dimensions: 2
layout {
  minor_to_major: 1
  minor_to_major: 0
  format: DENSE
}
is_dynamic_dimension: false
is_dynamic_dimension: false

[FTXJ LOG] BuildRelu call xla::Max, add inst to Builder
[FTXJ LOG] End BuildRelu
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext call LowerNodeSyncTensorsGraph
[FTXJ LOG] LoweringContext::LowerNode. cast (Lazy node) -> (XlaOpVector) and do lower
[FTXJ LOG] LoweringContext::LowerNode call Lower
[FTXJ LOG] NodePtr::Add Constructor lower_fn
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op0
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] NodePtr::Add Constructor lower_fn call GetOutputOp Op1
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp
[FTXJ LOG] XlaHelpers::ShapeOfXlaOp End
[ReturnOp] call. Also Loctx AssignOutputOp.
[FTXJ LOG] LoweringContext::AssignOutputOp assign output->op pointer
[FTXJ LOG] LoweringContext::AssignOutputOp End
[FTXJ LOG] LoweringContext::LowerNode End
[FTXJ LOG] LoweringContext::LoweringContext EndSyncTensorsGraph
[FTXJ LOG] XLATensor::Compiler mid info
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
index = 0, XlaOp = 58
[FTXJ LOG] LoweringContext::AddResult End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
index = 1, XlaOp = 115
[FTXJ LOG] LoweringContext::AddResult End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
index = 2, XlaOp = 118
[FTXJ LOG] LoweringContext::AddResult End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
index = 3, XlaOp = 119
[FTXJ LOG] LoweringContext::AddResult End
[FTXJ LOG] LoweringContext::GetOutputOp
find output in emitted_outputs_ context, if not, build post_order, and lower, then get
[FTXJ LOG] LoweringContext::GetOutputOp End
index = 4, XlaOp = 126
[FTXJ LOG] LoweringContext::AddResult End
[FTXJ LOG] LoweringContext::GetParametersData End
[FTXJ LOG] LoweringContext::Build
[FTXJ MSG] build xla::XlaComputation from nodes
[FTXJ LOG] LoweringContext::Build from root
[FTXJ LOG] LoweringContext::Build End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::Compile call xla::ComputationClient::Compile
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ MSG] Compiling IR graph hash 37f4efc56eace7204eda7f180b4cda2d on device CPU:0 done![FTXJ MSG] Graph hash 37f4efc56eace7204eda7f180b4cda2d is computation hash cb18f4db326e1e871332416b42421cdc[FTXJ MSG] Compiling En. then call GetEmittedNodeCount
[FTXJ LOG] SyncTensorsGraphInternal call GetComputationCache()->Add
[FTXJ LOG] SyncTensorsGraphInternal call GetComputationCache()->Add
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::SetXlaData with sync
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetXlaData with sync End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::SetXlaData with sync
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetXlaData with sync End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::SetXlaData with sync
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetXlaData with sync End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::SetXlaData with sync
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetXlaData with sync End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XLATensor::SetXlaData with sync
[FTXJ LOG] XLATensor::AssignIrValue
[FTXJ LOG] XLATensor::AssignIrValue End
[FTXJ LOG] XLATensor::SetXlaData with sync End
[FTXJ LOG] [ScheduleSyncTensorsGraph]
call XLATensor::TensorCollectionBarrier
[FTXJ LOG] SyncTensorsGraphInternal End
[SyncTensorsGraph End]
[SyncLiveTensorsGraph End]
[XLATensor::MarkStep Begin]
[FTXJ LOG] DeviceContextArena::MarkStep.
[FTXJ LOG] DeviceContextArena::MarkStep End
[XLATensor::MarkStep End]
[StepMarker] End
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] XrtComputationClient::ExecuteComputation
[FTXJ LOG] XrtComputationClient::ExecuteComputation call CreateExecuteOps
[GetCurrentAtenDevice]
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
[FTXJ LOG] XrtComputationClient::SetupExecConfig
[FTXJ LOG] XrtComputationClient::SetupExecConfig End
[GetCurrentAtenDevice]
[XlaDeviceToAtenDevice]
[FTXJ LOG] AtenXlaDeviceMapper::Get
[FTXJ LOG] GetDeviceOrdinal End. from device to get index
1------------------------------------------------------------------
[FTXJ LOG] xla_device. xla_model.py
[FTXJ LOG] xla_device call _xla_set_default_device
[FTXJ LOG] xla_device call torch.device
[FTXJ LOG] xla_device End
2------------------------------------------------------------------
3------------------------------------------------------------------
4------------------------------------------------------------------
5------------------------------------------------------------------
6------------------------------------------------------------------
7------------------------------------------------------------------
8------------------------------------------------------------------
[XLATensor::WaitDeviceOps Begin]
[FTXH LOG] ComputationClient::Get. singleton
[FTXJ LOG] DeviceType default construction CPU
[FTXJ LOG] XrtComputationClient::ExecuteComputation call GetComputationResults
[FTXJ LOG] XrtComputationClient::ExecuteComputation End
[XLATensor::WaitDeviceOps End]
[FTXJ LOG] Deconstructor XLATensor::Data
[FTXJ LOG] Unregister Tensor to Device. erase data in device vector
[FTXJ LOG] Deconstructor XLATensor::Data End
[FTXJ LOG] Deconstructor XLATensor::Data
[FTXJ LOG] Unregister Tensor to Device. erase data in device vector
[FTXJ LOG] Deconstructor XLATensor::Data End
[FTXJ LOG] Deconstructor XLATensor::Data
[FTXJ LOG] Unregister Tensor to Device. erase data in device vector
[FTXJ LOG] Deconstructor XLATensor::Data End
[FTXJ LOG] Deconstructor XLATensor::Data
[FTXJ LOG] Unregister Tensor to Device. erase data in device vector
[FTXJ LOG] Deconstructor XLATensor::Data End
[FTXJ LOG] Deconstructor XLATensor::Data
[FTXJ LOG] Unregister Tensor to Device. erase data in device vector
[FTXJ LOG] Deconstructor XLATensor::Data End